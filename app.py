import logging
logging.basicConfig(level=logging.DEBUG)
from flask import Flask, request, jsonify
import requests
import random
from datetime import datetime
import logging
import urllib3
import os  

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# ===== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø =====
GEMINI_KEYS = [key.strip().strip('"').strip("'") for key in os.getenv('GEMINI_KEYS', '').split(',') if key.strip()]
YOUR_MASTER_KEY = os.getenv('MASTER_KEY', '')

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–π
if not GEMINI_KEYS or GEMINI_KEYS == ['']:
    logger.error("‚ùå No Gemini keys found in environment variables")
    exit(1)
    
if not YOUR_MASTER_KEY:
    logger.error("‚ùå No master key found in environment variables") 
    exit(1)

key_usage = {key: {'requests': 0, 'last_used': None, 'errors': 0} for key in GEMINI_KEYS}

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Gemini 2.5 Pro
GEMINI_MODEL = "gemini-2.5-pro"
MAX_INPUT_TOKENS = 1048576    
MAX_OUTPUT_TOKENS = 65536     
DEFAULT_OUTPUT_TOKENS = 60000  

# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
DETAILED_INSTRUCTION = """You are PROHIBITED from describing the thoughts, actions and any words of {{user}}. You are only allowed to describe the consequences of {{user}}'s actions or characters that perform some action on the {{user}}. You must NEVER write {{user}}'s dialogues, actions and thoughts under any circumstances, as they are off-limits; you only write for {{char}} and others.
Write EXTREMELY DETAILED and expansive responses in RUSSIAN.
Minimum response length: 3000+ words.
Describe: details, emotions, environment, dialogues, sensations, character thoughts.
Use rich vocabulary and literary style.
Always develop the story and add new narrative elements.
Focus on sensory descriptions - smells, sounds, textures, tastes.
Include internal monologues and character reflections.
Create immersive atmospheric descriptions.
Expand on subplots and secondary characters.
Never be concise - always elaborate and expand.You are required to adhere to the following
settings.
"""

class KeyBalancer:
    def get_best_key(self):
        available_keys = [k for k, v in key_usage.items() if v['errors'] < 3]
        if not available_keys:
            available_keys = GEMINI_KEYS
        key = min(available_keys, key=lambda k: key_usage[k]['requests'])
        # –û—á–∏—Å—Ç–∫–∞ –∫–ª—é—á–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        key = key.strip().strip('"').strip("'")
        return key

balancer = KeyBalancer()

# ===== ENDPOINT –î–õ–Ø SILLYTAVERN =====
@app.route('/v1/models', methods=['GET'])
def list_models():
    return jsonify({
        "object": "list",
        "data": [
            {
                "id": GEMINI_MODEL,
                "object": "model", 
                "created": 1686935000,
                "owned_by": "google",
                "limits": {
                    "max_input_tokens": MAX_INPUT_TOKENS,
                    "max_output_tokens": MAX_OUTPUT_TOKENS
                }
            }
        ]
    })

# ===== ENDPOINTS –î–õ–Ø JANITORAI =====
@app.route('/v1/engines', methods=['GET'])
def list_engines():
    return list_models()

@app.route('/v1/completions', methods=['POST'])
def completions():
    # JanitorAI —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç, –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–¥–∞–µ–º –≤ chat_completions
    logger.info("JanitorAI using chat format, redirecting to chat_completions")
    return chat_completions()

# ===== OPENAI-–°–û–í–ú–ï–°–¢–ò–ú–´–ô API =====
@app.route('/v1/chat/completions', methods=['POST', 'OPTIONS'])
def chat_completions():
    if request.method == 'OPTIONS':
        return '', 200
        
    try:
        data = request.json
        if not data or 'messages' not in data:
            return jsonify({"error": "Invalid request format"}), 400
        
        gemini_key = balancer.get_best_key()
        key_usage[gemini_key]['requests'] += 1
        key_usage[gemini_key]['last_used'] = datetime.now().isoformat()
        
        logger.info(f"Using key: {gemini_key[:20]}... | Requests: {key_usage[gemini_key]['requests']}")
        
        contents = []
        system_instruction = DETAILED_INSTRUCTION

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π
        for msg in data["messages"]:
            role = "user" if msg["role"] == "user" else "model"
            contents.append({
                "role": role,
                "parts": [{"text": msg["content"]}]
            })

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º max_tokens —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–æ–≤ Gemini
        requested_tokens = data.get("max_tokens", DEFAULT_OUTPUT_TOKENS)
        max_output_tokens = min(requested_tokens, MAX_OUTPUT_TOKENS)
        
        gemini_data = {
            "contents": contents,
            "system_instruction": {
                "parts": [{"text": system_instruction}]
            },
            "generationConfig": {
                "temperature": data.get("temperature", 0.9),  # –£–≤–µ–ª–∏—á–∏–ª –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                "maxOutputTokens": max_output_tokens,
                "topP": data.get("top_p", 0.95),            # –£–≤–µ–ª–∏—á–∏–ª –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                "topK": data.get("top_k", 60),              # –£–≤–µ–ª–∏—á–∏–ª –¥–ª—è –±–æ–≥–∞—Ç—Å—Ç–≤–∞ –≤—ã–±–æ—Ä–∞
            },
            "safetySettings": [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH",
                    "threshold": "BLOCK_NONE" 
                },
                {
                    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                    "threshold": "BLOCK_NONE"
                }
            ]
        }
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ –∫ Gemini 2.5 Pro
        response = requests.post(
            f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent?key={gemini_key}",
            json=gemini_data,
            headers={'Content-Type': 'application/json'},
            timeout=120,
            verify=False
        )
        
        if response.status_code == 429:
            key_usage[gemini_key]['errors'] += 1
            logger.warning(f"Rate limit for key: {gemini_key[:20]}...")
            return chat_completions()
            
        if response.status_code != 200:
            key_usage[gemini_key]['errors'] += 1
            logger.error(f"Gemini API error {response.status_code} for key: {gemini_key[:20]}...")
            return jsonify({"error": f"Gemini API error: {response.status_code}"}), 500
            
        gemini_response = response.json()
        
        if ('candidates' not in gemini_response or 
            not gemini_response['candidates'] or 
            'content' not in gemini_response['candidates'][0] or
            'parts' not in gemini_response['candidates'][0]['content']):
            logger.error(f"Invalid Gemini response structure: {gemini_response}")
            return jsonify({"error": "Invalid response from Gemini API"}), 500
            
        response_text = gemini_response["candidates"][0]["content"]["parts"][0]["text"]
        
        total_input_chars = sum(len(msg["content"]) for msg in data["messages"])
        
        openai_format = {
            "id": f"chatcmpl-{random.randint(1000,9999)}",
            "object": "chat.completion", 
            "created": int(datetime.now().timestamp()),
            "model": GEMINI_MODEL,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": total_input_chars // 4,
                "completion_tokens": len(response_text) // 4,
                "total_tokens": (total_input_chars + len(response_text)) // 4
            }
        }
        
        logger.info(f"‚úÖ Success! Input: {total_input_chars} chars, Output: {len(response_text)} chars, Max tokens: {max_output_tokens}")
        return jsonify(openai_format)
        
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        return jsonify({"error": str(e)}), 500

# ===== –ê–£–¢–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–Ø =====
@app.before_request
def authenticate():
    # –†–∞–∑—Ä–µ—à–∞–µ–º OPTIONS –∑–∞–ø—Ä–æ—Å—ã –±–µ–∑ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    if request.method == 'OPTIONS':
        return None
        
    if request.endpoint in ['chat_completions', 'list_models', 'completions']:
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            return jsonify({"error": "Authorization header required"}), 401
        api_key = auth_header.replace('Bearer ', '')
        if api_key != YOUR_MASTER_KEY:
            return jsonify({"error": "Invalid API key"}), 401

# ===== CORS =====
@app.after_request
def after_request(response):
    response.headers.add('Access-Control-Allow-Origin', '*')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')
    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')
    return response

# ===== –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ú–û–î–ï–õ–ò =====
@app.route('/v1/model-info', methods=['GET'])
def model_info():
    return jsonify({
        "model": GEMINI_MODEL,
        "max_input_tokens": MAX_INPUT_TOKENS,
        "max_output_tokens": MAX_OUTPUT_TOKENS,
        "features": ["large_context", "multimodal", "reasoning"]
    })

if __name__ == '__main__':
    print("üöÄ Production Gemini Proxy starting...")
    print(f"üìä Available keys: {len(GEMINI_KEYS)}")
    print(f"üîë Your master key: {YOUR_MASTER_KEY}")
    print(f"ü§ñ Model: {GEMINI_MODEL}")
    print(f"üìñ Context: {MAX_INPUT_TOKENS:,} tokens")
    print(f"üìù Output: {MAX_OUTPUT_TOKENS:,} tokens")
    print("üìç Endpoint: http://localhost:5000/v1")
    print("üéØ Production server: Waitress")
    from waitress import serve
    serve(app, host='0.0.0.0', port=5000, threads=10)
